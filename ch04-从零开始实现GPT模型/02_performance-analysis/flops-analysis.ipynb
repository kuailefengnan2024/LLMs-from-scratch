{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbrESHKtgzPA"
   },
   "source": [
    "# FLOPS 分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS2WjniMgzPB"
   },
   "source": [
    "- FLOPs（每秒浮点运算次数）通过计算执行的浮点运算次数来衡量神经网络模型的计算复杂度\n",
    "- 高 FLOPs 表示更密集的计算和能耗"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L01-NzkggzPB"
   },
   "outputs": [],
   "source": "# pip install -r requirements-extra.txt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObzfVatqgzPC",
    "outputId": "3ead6a41-ac38-4db1-9fc3-012fb3ad18cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thop version: 0.1.1-2209072238\n",
      "torch version: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"thop\",\n",
    "    \"torch\",\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74UpjSLjgzPC"
   },
   "source": [
    "&nbsp;\n",
    "# 固定批大小的简单基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90pnCK39gzPD"
   },
   "source": [
    "- 仅前向传播"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GerIdRMXd6g9",
    "outputId": "177c6d00-a817-40fe-badd-95cfa8ac9b51"
   },
   "outputs": [],
   "source": "import torch\nfrom thop import profile\n\n# 安装说明请参见：\n# https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\nfrom llms_from_scratch.ch04 import GPTModel\n\n\nBASE_CONFIG = {\n    \"vocab_size\": 50257,     # 词汇表大小\n    \"context_length\": 1024,  # 上下文长度\n    \"drop_rate\": 0.0,        # Dropout率\n    \"qkv_bias\": True         # 查询-键-值偏置\n}\n\nmodel_configs = {\n    \"gpt-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 2\ninput_tensor = torch.randint(0, 50257, (batch_size, 1024)).to(device)\n\nfor size in model_configs:\n    BASE_CONFIG.update(model_configs[size])\n\n    model = GPTModel(BASE_CONFIG).bfloat16()\n    model.to(device)\n\n    # MACS = 乘法累加运算\n    # MACS 通常计算为两个 FLOPS（一个乘法和一个累加）\n    macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n    flops = 2*macs\n    print(f\"{size:18}: {flops:.1e} FLOPS\")\n\n    del model\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S6V05QmgzPD"
   },
   "source": [
    "&nbsp;\n",
    "# 自动批大小查找的简单基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amw4E983gzPD"
   },
   "source": [
    "- 仅前向传播"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h08VOiqpgzPE",
    "outputId": "a6a90ef8-28fb-4b55-9268-6915b0c84c51"
   },
   "outputs": [],
   "source": "for size in model_configs:\n    print(f\"\\n处理 {size}\")\n    config = BASE_CONFIG.copy()\n    config.update(model_configs[size])\n\n    min_batch_size = 1\n    max_batch_size = None\n    max_possible_batch_size = 4096\n\n    while min_batch_size <= max_possible_batch_size:\n        batch_size = (min_batch_size + max_possible_batch_size) // 2\n        try:\n            input_tensor = torch.randint(\n                0, config[\"vocab_size\"],\n                (batch_size, config[\"context_length\"]),\n                device=device\n            )\n\n            model = GPTModel(config).bfloat16().to(device)\n\n            # MACS = 乘法累加运算\n            # MACS 通常计算为两个 FLOPS（一个乘法和一个累加）\n            macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n            flops = 2 * macs\n            print(f\"  批大小 {batch_size}: {flops:.1e} FLOPS\")\n\n            # 如果成功，尝试更大的批大小\n            min_batch_size = batch_size + 1\n            max_batch_size = batch_size\n\n            # 清理内存\n            del model, input_tensor\n            torch.cuda.empty_cache()\n\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                # 尝试更小的批大小\n                max_possible_batch_size = batch_size - 1\n\n                # 清理内存\n                try:\n                    del model, input_tensor\n                    torch.cuda.empty_cache()\n                except NameError:\n                    pass\n            else:\n                raise e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4lD7tfcgzPE"
   },
   "source": [
    "&nbsp;\n",
    "# 具有自动批大小查找和模型 FLOP 利用率 (MFU) 的基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70Y2mblVgzPE"
   },
   "source": [
    "- 来自 [PaLM 论文](https://arxiv.org/abs/2204.02311) 的模型 FLOPs 利用率 (MFU) 解释\n",
    "\n",
    "> 我们提出了一个新的效率指标，它与实现无关，允许更清晰地比较系统效率，称为模型 FLOPs 利用率 (MFU)。这是观察到的吞吐量（每秒令牌数）相对于在峰值 FLOPs 下运行的系统理论最大吞吐量的比率。关键是，\"理论最大\"吞吐量只考虑计算前向+反向传播所需的运算，而不考虑重新计算。\n",
    "\n",
    "\n",
    "$$\\text{MFU} = \\frac{\\text{观察到的每秒令牌数}}{\\text{理论最大每秒令牌数}}$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\\text{理论最大每秒令牌数} = \\frac{\\text{每秒最大 FLOPs}}{\\text{每个令牌的总 FLOPs}}$$\n",
    "\n",
    "以及\n",
    "\n",
    "$$\\text{每秒令牌数} = \\frac{\\text{批大小} \\times \\text{序列长度}}{\\text{总时间}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKttjC8xgzPF"
   },
   "source": [
    "- 前向和反向传播"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6aO4rjtNgzPF"
   },
   "outputs": [],
   "source": "# GPU 制造商提供的理论最大每秒 FLOPS\n\nflops_per_second = {\n    # https://www.techpowerup.com/gpu-specs/h100-pcie-80-gb.c3899\n    \"H100\": {\n        torch.float32: 51.22e12,  # NVIDIA H100 FP32 为 51.22 TFLOPs\n        torch.float16: 204.9e12,  # NVIDIA H100 FP16 为 204.9 TFLOPs\n        torch.bfloat16: 204.9e12\n    },\n    # https://www.techpowerup.com/gpu-specs/l4.c4091\n    \"L4\": {\n        torch.float32: 30.29e12,  # NVIDIA L4 FP32 为 30.29 TFLOPs\n        torch.float16: 30.29e12,  # NVIDIA L4 FP16 为 30.29 TFLOPs\n        torch.bfloat16: 30.29e12\n    },\n    # https://www.techpowerup.com/gpu-specs/tesla-t4.c3316\n    \"T4\": {\n        torch.float32: 8.1e12,  # NVIDIA T4 FP32 为 8.1 TFLOPs\n        torch.float16: 65.13e12,  # NVIDIA T4 FP16 为 65.13 TFLOPs\n        torch.bfloat16: 65.13e12\n    },\n    # https://www.techpowerup.com/gpu-specs/a10g.c3798\n    \"A10G\": {\n        torch.float32: 31.52e12,  # NVIDIA A10G FP32 为 31.52 TFLOPs\n        torch.float16: 31.52e12,  # NVIDIA A10G FP16 为 31.52 TFLOPs\n        torch.bfloat16: 31.52e12\n    },\n    # https://www.techpowerup.com/gpu-specs/a100-pcie-40-gb.c3623\n    \"A100\": {\n        torch.float32: 19.49e12,  # NVIDIA A100 FP32 为 19.49 TFLOPs\n        torch.float16: 77.97e12,  # NVIDIA A100 FP16 为 77.97 TFLOPs\n        torch.bfloat16: 77.97e12\n    },\n    # https://www.techpowerup.com/gpu-specs/geforce-rtx-3080.c3621\n    \"RTX_3080\": {\n        torch.float32: 29.77e12,  # NVIDIA RTX 3080 FP32 为 29.77 TFLOPs\n        torch.float16: 29.77e12,  # NVIDIA RTX 3080 FP16 为 29.77 TFLOPs\n        torch.bfloat16: 29.77e12\n    },\n    # https://www.techpowerup.com/gpu-specs/geforce-rtx-3090.c3622\n    \"RTX_3090\": {\n        torch.float32: 35.58e12,  # NVIDIA RTX 3090 FP32 为 35.58 TFLOPs\n        torch.float16: 35.58e12,  # NVIDIA RTX 3090 FP16 为 35.58 TFLOPs\n        torch.bfloat16: 35.58e12\n    }\n}"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "HW5qWfE7gzPF",
    "outputId": "bb1663bc-ee66-44f1-f54d-0bb66ee0d0c2"
   },
   "outputs": [],
   "source": "import time\n\ndef get_gpu_model(flops_per_second_dict):\n    device_name = torch.cuda.get_device_name(0)\n    for model in flops_per_second_dict.keys():\n        if model in device_name:\n            return model\n    return \"Unknown\"  # 如果没有找到匹配的模型，返回默认值\n\n\ngpu_model = get_gpu_model(flops_per_second)\nprint(\"GPU 模型:\", gpu_model)\n\nif gpu_model != \"Unknown\":\n\n    for size in model_configs:\n        print(f\"\\n处理 {size}\")\n        config = BASE_CONFIG.copy()\n        config.update(model_configs[size])\n\n        min_batch_size = 1\n        max_batch_size = None\n        max_possible_batch_size = 4096\n\n        while min_batch_size <= max_possible_batch_size:\n            batch_size = (min_batch_size + max_possible_batch_size) // 2\n            try:\n                input_tensor = torch.randint(\n                    0, config[\"vocab_size\"],\n                    (batch_size, config[\"context_length\"]),\n                    device=device\n                )\n\n                model = GPTModel(config).bfloat16().to(device)\n                model.train()\n\n                # 开始计时\n                torch.cuda.synchronize()\n                start_time = time.time()\n\n                # 前向 & 反向传播\n                output = model(input_tensor)\n                loss = output.sum()  # 计算一个虚拟损失\n                loss.backward()\n\n                # 结束计时\n                torch.cuda.synchronize()\n                end_time = time.time()\n\n                total_time_seconds = end_time - start_time\n\n                # 计算前向传播的 FLOPs\n                macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n                flops_forward = 2 * macs  # 假设一个 MAC 等于两个 FLOPs\n\n                # 估计反向传播的 FLOPs（通常是前向 FLOPs 的 2 倍）\n                flops_backward = 2 * flops_forward\n\n                # 前向 + 反向传播的总 FLOPs\n                total_flops = flops_forward + flops_backward  # 或者 total_flops = flops_forward * 3\n\n                data_type = next(model.parameters()).dtype\n                max_flops_per_second = flops_per_second[gpu_model].get(data_type, 0)\n\n                # 计算每秒令牌数\n                tokens_processed = batch_size * config[\"context_length\"]\n                tokens_per_second = tokens_processed / total_time_seconds\n\n                # 计算每个令牌的 FLOPs\n                flops_per_token = total_flops / tokens_processed\n\n                # 计算理论最大每秒令牌数\n                if flops_per_token > 0:\n                    theoretical_max_tokens_per_second = max_flops_per_second / flops_per_token\n                else:\n                    theoretical_max_tokens_per_second = 0  # 避免除零\n\n                # 计算 MFU\n                if theoretical_max_tokens_per_second > 0:\n                    mfu = tokens_per_second / theoretical_max_tokens_per_second\n                else:\n                    mfu = 0  # 避免除零\n\n                print(f\"  批大小 {batch_size}: 令牌/秒: {tokens_per_second:.2f}, MFU: {mfu:.4f}\")\n\n                # 如果成功，尝试更大的批大小\n                min_batch_size = batch_size + 1\n                max_batch_size = batch_size\n\n                # 清理内存\n                del model, input_tensor, output, loss\n                torch.cuda.empty_cache()\n\n            except RuntimeError as e:\n                if \"out of memory\" in str(e).lower():\n                    # 尝试更小的批大小\n                    max_possible_batch_size = batch_size - 1\n\n                    # 清理内存\n                    try:\n                        del model, input_tensor\n                        torch.cuda.empty_cache()\n                    except NameError:\n                        pass\n                else:\n                    raise e\n\nelse:\n    print(\"未知的 GPU 模型。请使用您的 GPU 信息更新 flops_per_second 字典。\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LovmswRigzPG"
   },
   "source": "- 值为 1.0 是最好的（等于 100%）\n- 注意批大小比之前更小，因为我们这里也执行反向传播，这需要更多内存",
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}